{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\nfrom tensorflow.keras.activations import relu, sigmoid\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom keras.optimizers import Adam\nimport tensorflow as tf\nimport os\nimport math\nimport numpy as np\nimport cv2 as cv\nimport random as rd\nimport matplotlib.pyplot as plt\n\n\nrd.seed(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:23:26.719160Z","iopub.execute_input":"2023-05-31T01:23:26.720280Z","iopub.status.idle":"2023-05-31T01:23:26.727984Z","shell.execute_reply.started":"2023-05-31T01:23:26.720233Z","shell.execute_reply":"2023-05-31T01:23:26.726637Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=(96, 96, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(3, 3),  # Menggunakan filter pooling yang lebih besar\n    tf.keras.layers.Dropout(0.25),\n\n    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.25),\n\n    tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.25),\n\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(2048, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(17, activation='sigmoid')\n])\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:23:33.409846Z","iopub.execute_input":"2023-05-31T01:23:33.410224Z","iopub.status.idle":"2023-05-31T01:23:34.127825Z","shell.execute_reply.started":"2023-05-31T01:23:33.410194Z","shell.execute_reply":"2023-05-31T01:23:34.126528Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class FashionDataset(Sequence):\n    def __init__(self, paths: list, batch_size: int):\n        super(FashionDataset, self).__init__()\n        self.batch_size = batch_size\n        self.data_paths = paths\n        self.classes = []\n        self.labels = self._generate_labels(self.data_paths)\n\n    def __len__(self):\n        return math.ceil(len(self.data_paths) / self.batch_size)\n\n    def __getitem__(self, idx):\n        batch_x = self.data_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n        return np.array([self._load_image(i) for i in batch_x]), np.array(batch_y)\n\n    def _generate_labels(self, paths: list):\n        labels = []\n        for path in paths:\n            label = path.split(os.path.sep)[-2].split('_')\n            labels.append(label)\n\n        mlb = MultiLabelBinarizer()\n        labels = mlb.fit_transform(labels)\n        self.classes = mlb.classes_\n\n        return labels\n\n    @staticmethod\n    def _load_image(path: str):\n        img = cv.imread(path)\n        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n        img = cv.resize(img, (96, 96))\n        return img / 255.0\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:23:39.499543Z","iopub.execute_input":"2023-05-31T01:23:39.499937Z","iopub.status.idle":"2023-05-31T01:23:39.512822Z","shell.execute_reply.started":"2023-05-31T01:23:39.499899Z","shell.execute_reply":"2023-05-31T01:23:39.511860Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_sz = 0.75\npaths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname, filename))\n\npaths = sorted(paths)\nrd.shuffle(paths)\nassert 0.0 < train_sz <= 1.0\nthresh = round(len(paths) * train_sz)\ntrain_paths = paths[:thresh]\ntest_paths = paths[thresh:]\nprint(train_paths[0], test_paths[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:23:44.995802Z","iopub.execute_input":"2023-05-31T01:23:44.996251Z","iopub.status.idle":"2023-05-31T01:23:45.128283Z","shell.execute_reply.started":"2023-05-31T01:23:44.996215Z","shell.execute_reply":"2023-05-31T01:23:45.127158Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/apparel-dataset/blue_shirt/102.jpg /kaggle/input/apparel-dataset/black_shorts/31.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"train_x = FashionDataset(train_paths, batch_size=32)\ntest_y = FashionDataset(test_paths, batch_size=32)\n\nmodel.compile(\n    loss=tf.keras.losses.MAE,\n    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3, decay=0.00005),\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:23:49.349698Z","iopub.execute_input":"2023-05-31T01:23:49.350688Z","iopub.status.idle":"2023-05-31T01:23:49.430958Z","shell.execute_reply.started":"2023-05-31T01:23:49.350648Z","shell.execute_reply":"2023-05-31T01:23:49.429830Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for i in range(len(train_x.classes)):\n    classes = '\\n'.join(train_x.classes)\n\nwith open('classes.txt', 'w') as f:\n    f.write(classes)\n    \nmodel_json = model.to_json()\n\nwith open('model.json', 'w') as f:\n    f.write(model_json)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:23:53.634954Z","iopub.execute_input":"2023-05-31T01:23:53.635374Z","iopub.status.idle":"2023-05-31T01:23:53.647506Z","shell.execute_reply.started":"2023-05-31T01:23:53.635339Z","shell.execute_reply":"2023-05-31T01:23:53.646298Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"best_w = ModelCheckpoint('best.h5',\n                        monitor='val_loss',\n                        verbose=1,\n                        save_weights_only=True,\n                        save_best_only=True,\n                        mode='min',\n                        save_freq='epoch')\n\nlast_w = ModelCheckpoint('last.h5',\n                        monitor='val_loss',\n                        verbose=1,\n                        save_weights_only=True,\n                        save_best_only=False,\n                        mode='auto',\n                        save_freq='epoch')","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:24:37.133883Z","iopub.execute_input":"2023-05-31T01:24:37.134290Z","iopub.status.idle":"2023-05-31T01:24:37.141289Z","shell.execute_reply.started":"2023-05-31T01:24:37.134246Z","shell.execute_reply":"2023-05-31T01:24:37.139990Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"with tf.device('/GPU:0'):\n    history = model.fit(x=train_x,\n                        validation_data=test_y,\n                        steps_per_epoch=len(train_x),\n                        epochs=100,\n                        callbacks=[best_w, last_w])","metadata":{"execution":{"iopub.status.busy":"2023-05-31T01:24:43.298456Z","iopub.execute_input":"2023-05-31T01:24:43.298860Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n379/379 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.3994\nEpoch 1: val_loss improved from inf to 0.09254, saving model to best.h5\n\nEpoch 1: saving model to last.h5\n379/379 [==============================] - 450s 1s/step - loss: 0.1246 - accuracy: 0.3994 - val_loss: 0.0925 - val_accuracy: 0.2147\nEpoch 2/100\n379/379 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.4806\nEpoch 2: val_loss improved from 0.09254 to 0.05213, saving model to best.h5\n\nEpoch 2: saving model to last.h5\n379/379 [==============================] - 434s 1s/step - loss: 0.0491 - accuracy: 0.4806 - val_loss: 0.0521 - val_accuracy: 0.5136\nEpoch 3/100\n379/379 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.5087\nEpoch 3: val_loss did not improve from 0.05213\n\nEpoch 3: saving model to last.h5\n379/379 [==============================] - 430s 1s/step - loss: 0.0422 - accuracy: 0.5087 - val_loss: 0.0549 - val_accuracy: 0.4822\nEpoch 4/100\n379/379 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.5245\nEpoch 4: val_loss improved from 0.05213 to 0.03547, saving model to best.h5\n\nEpoch 4: saving model to last.h5\n379/379 [==============================] - 433s 1s/step - loss: 0.0375 - accuracy: 0.5245 - val_loss: 0.0355 - val_accuracy: 0.5713\nEpoch 5/100\n379/379 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.5486\nEpoch 5: val_loss did not improve from 0.03547\n\nEpoch 5: saving model to last.h5\n379/379 [==============================] - 434s 1s/step - loss: 0.0347 - accuracy: 0.5486 - val_loss: 0.0440 - val_accuracy: 0.5891\nEpoch 6/100\n379/379 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.5592\nEpoch 6: val_loss did not improve from 0.03547\n\nEpoch 6: saving model to last.h5\n379/379 [==============================] - 434s 1s/step - loss: 0.0324 - accuracy: 0.5592 - val_loss: 0.0650 - val_accuracy: 0.5475\nEpoch 7/100\n379/379 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.5684\nEpoch 7: val_loss improved from 0.03547 to 0.03353, saving model to best.h5\n\nEpoch 7: saving model to last.h5\n379/379 [==============================] - 433s 1s/step - loss: 0.0309 - accuracy: 0.5684 - val_loss: 0.0335 - val_accuracy: 0.5856\nEpoch 8/100\n379/379 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.5832\nEpoch 8: val_loss improved from 0.03353 to 0.02914, saving model to best.h5\n\nEpoch 8: saving model to last.h5\n379/379 [==============================] - 432s 1s/step - loss: 0.0289 - accuracy: 0.5832 - val_loss: 0.0291 - val_accuracy: 0.6301\nEpoch 9/100\n379/379 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.6103\nEpoch 9: val_loss improved from 0.02914 to 0.02908, saving model to best.h5\n\nEpoch 9: saving model to last.h5\n379/379 [==============================] - 430s 1s/step - loss: 0.0270 - accuracy: 0.6103 - val_loss: 0.0291 - val_accuracy: 0.6675\nEpoch 10/100\n379/379 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.6210\nEpoch 10: val_loss improved from 0.02908 to 0.02782, saving model to best.h5\n\nEpoch 10: saving model to last.h5\n379/379 [==============================] - 442s 1s/step - loss: 0.0261 - accuracy: 0.6210 - val_loss: 0.0278 - val_accuracy: 0.6690\nEpoch 11/100\n379/379 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.6229\nEpoch 11: val_loss improved from 0.02782 to 0.02685, saving model to best.h5\n\nEpoch 11: saving model to last.h5\n379/379 [==============================] - 434s 1s/step - loss: 0.0256 - accuracy: 0.6229 - val_loss: 0.0268 - val_accuracy: 0.6492\nEpoch 12/100\n379/379 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.6282\nEpoch 12: val_loss did not improve from 0.02685\n\nEpoch 12: saving model to last.h5\n379/379 [==============================] - 436s 1s/step - loss: 0.0247 - accuracy: 0.6282 - val_loss: 0.0391 - val_accuracy: 0.6598\nEpoch 13/100\n379/379 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.6331\nEpoch 13: val_loss improved from 0.02685 to 0.02593, saving model to best.h5\n\nEpoch 13: saving model to last.h5\n379/379 [==============================] - 435s 1s/step - loss: 0.0239 - accuracy: 0.6331 - val_loss: 0.0259 - val_accuracy: 0.6727\nEpoch 14/100\n379/379 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.6350\nEpoch 14: val_loss did not improve from 0.02593\n\nEpoch 14: saving model to last.h5\n379/379 [==============================] - 434s 1s/step - loss: 0.0226 - accuracy: 0.6350 - val_loss: 0.0325 - val_accuracy: 0.6613\nEpoch 15/100\n379/379 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.6593\nEpoch 15: val_loss improved from 0.02593 to 0.02522, saving model to best.h5\n\nEpoch 15: saving model to last.h5\n379/379 [==============================] - 433s 1s/step - loss: 0.0217 - accuracy: 0.6593 - val_loss: 0.0252 - val_accuracy: 0.6856\nEpoch 16/100\n379/379 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.6609\nEpoch 16: val_loss did not improve from 0.02522\n\nEpoch 16: saving model to last.h5\n379/379 [==============================] - 433s 1s/step - loss: 0.0209 - accuracy: 0.6609 - val_loss: 0.0360 - val_accuracy: 0.6284\nEpoch 17/100\n379/379 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.6584\nEpoch 17: val_loss did not improve from 0.02522\n\nEpoch 17: saving model to last.h5\n379/379 [==============================] - 433s 1s/step - loss: 0.0215 - accuracy: 0.6584 - val_loss: 0.0429 - val_accuracy: 0.5344\nEpoch 18/100\n379/379 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.6579\nEpoch 18: val_loss did not improve from 0.02522\n\nEpoch 18: saving model to last.h5\n379/379 [==============================] - 436s 1s/step - loss: 0.0210 - accuracy: 0.6579 - val_loss: 0.0296 - val_accuracy: 0.6771\nEpoch 19/100\n379/379 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.6800\nEpoch 19: val_loss did not improve from 0.02522\n\nEpoch 19: saving model to last.h5\n379/379 [==============================] - 431s 1s/step - loss: 0.0199 - accuracy: 0.6800 - val_loss: 0.0427 - val_accuracy: 0.5574\nEpoch 20/100\n379/379 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.6740\nEpoch 20: val_loss did not improve from 0.02522\n\nEpoch 20: saving model to last.h5\n379/379 [==============================] - 434s 1s/step - loss: 0.0202 - accuracy: 0.6740 - val_loss: 0.0260 - val_accuracy: 0.6687\nEpoch 21/100\n379/379 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.6727\nEpoch 21: val_loss did not improve from 0.02522\n\nEpoch 21: saving model to last.h5\n379/379 [==============================] - 441s 1s/step - loss: 0.0194 - accuracy: 0.6727 - val_loss: 0.0332 - val_accuracy: 0.6351\nEpoch 22/100\n379/379 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.6903\nEpoch 22: val_loss improved from 0.02522 to 0.02422, saving model to best.h5\n\nEpoch 22: saving model to last.h5\n379/379 [==============================] - 440s 1s/step - loss: 0.0187 - accuracy: 0.6903 - val_loss: 0.0242 - val_accuracy: 0.7021\nEpoch 23/100\n379/379 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.6898\nEpoch 23: val_loss did not improve from 0.02422\n\nEpoch 23: saving model to last.h5\n379/379 [==============================] - 434s 1s/step - loss: 0.0180 - accuracy: 0.6898 - val_loss: 0.0315 - val_accuracy: 0.6452\nEpoch 24/100\n379/379 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.7070\nEpoch 24: val_loss did not improve from 0.02422\n\nEpoch 24: saving model to last.h5\n379/379 [==============================] - 433s 1s/step - loss: 0.0181 - accuracy: 0.7070 - val_loss: 0.0327 - val_accuracy: 0.6875\nEpoch 25/100\n379/379 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.7010\nEpoch 25: val_loss did not improve from 0.02422\n\nEpoch 25: saving model to last.h5\n379/379 [==============================] - 436s 1s/step - loss: 0.0172 - accuracy: 0.7010 - val_loss: 0.0263 - val_accuracy: 0.6977\nEpoch 26/100\n379/379 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.6958\nEpoch 26: val_loss did not improve from 0.02422\n\nEpoch 26: saving model to last.h5\n379/379 [==============================] - 435s 1s/step - loss: 0.0174 - accuracy: 0.6958 - val_loss: 0.0265 - val_accuracy: 0.6977\nEpoch 27/100\n379/379 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.7062\nEpoch 27: val_loss improved from 0.02422 to 0.02373, saving model to best.h5\n\nEpoch 27: saving model to last.h5\n379/379 [==============================] - 433s 1s/step - loss: 0.0166 - accuracy: 0.7062 - val_loss: 0.0237 - val_accuracy: 0.6712\nEpoch 28/100\n379/379 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.7212\nEpoch 28: val_loss did not improve from 0.02373\n\nEpoch 28: saving model to last.h5\n379/379 [==============================] - 435s 1s/step - loss: 0.0164 - accuracy: 0.7212 - val_loss: 0.0326 - val_accuracy: 0.6415\nEpoch 29/100\n379/379 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.7141\nEpoch 29: val_loss did not improve from 0.02373\n\nEpoch 29: saving model to last.h5\n379/379 [==============================] - 432s 1s/step - loss: 0.0163 - accuracy: 0.7141 - val_loss: 0.0240 - val_accuracy: 0.7113\nEpoch 30/100\n379/379 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.7219\nEpoch 30: val_loss improved from 0.02373 to 0.02209, saving model to best.h5\n\nEpoch 30: saving model to last.h5\n379/379 [==============================] - 432s 1s/step - loss: 0.0159 - accuracy: 0.7219 - val_loss: 0.0221 - val_accuracy: 0.7425\nEpoch 31/100\n379/379 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.7361\nEpoch 31: val_loss did not improve from 0.02209\n\nEpoch 31: saving model to last.h5\n379/379 [==============================] - 442s 1s/step - loss: 0.0156 - accuracy: 0.7361 - val_loss: 0.0237 - val_accuracy: 0.7128\nEpoch 32/100\n379/379 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.7259\nEpoch 32: val_loss did not improve from 0.02209\n\nEpoch 32: saving model to last.h5\n379/379 [==============================] - 439s 1s/step - loss: 0.0153 - accuracy: 0.7259 - val_loss: 0.0224 - val_accuracy: 0.7345\nEpoch 33/100\n379/379 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.7422\nEpoch 33: val_loss did not improve from 0.02209\n\nEpoch 33: saving model to last.h5\n379/379 [==============================] - 439s 1s/step - loss: 0.0150 - accuracy: 0.7422 - val_loss: 0.0225 - val_accuracy: 0.7039\nEpoch 34/100\n379/379 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.7316","output_type":"stream"}]}]}